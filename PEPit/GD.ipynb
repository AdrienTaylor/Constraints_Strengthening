{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdcb04a-82d4-4ec2-9d54-01a755dd09d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install PEPit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7fc98-256b-4aab-a991-1d71db49a3a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Readme\n",
    "\n",
    "This notebook exemplifies the results of [1, Section 3.2] (and in particular those of [1, Proposition 6]) for the numerical analysis of gradient descent).\n",
    "\n",
    "> [1] Rubbens, Anne, and Taylor, Adrien B. \"One-point extensions of function and operator classes.\"\n",
    "\n",
    "The code requires the installation of the [PEPit](https://pepit.readthedocs.io/en/latest/) package, e.g., by uncommenting the pip install command above.\n",
    "\n",
    "# Function class: refined characterization of convex functions that are smooth and satisfy a Lojasiewicz condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c1d512-b67d-4d92-9d2c-33a01a0ca5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PEPit import PEP\n",
    "from PEPit.function import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fde2c01-2e15-46d9-9cee-3b7709771ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImprovedLojasiewiczSmoothConvexFunction(Function):\n",
    "    def __init__(self,\n",
    "                 L,\n",
    "                 mu,\n",
    "                 alpha,\n",
    "                 is_leaf=True,\n",
    "                 decomposition_dict=None,\n",
    "                 reuse_gradient=True,\n",
    "                 name=None):\n",
    "        \n",
    "        super().__init__(is_leaf=is_leaf,\n",
    "                         decomposition_dict=decomposition_dict,\n",
    "                         reuse_gradient=True,\n",
    "                         name=name,\n",
    "                         )\n",
    "        assert L >= 0\n",
    "        assert mu >= 0\n",
    "        assert L >= mu\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.L = L\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def set_LojaSimple(self,\n",
    "                       xi, gi, fi,\n",
    "                       xj, gj, fj,\n",
    "                      ):\n",
    "        \n",
    "        constraint = (fi - fj <= gi**2 / 2 / self.mu)\n",
    "\n",
    "        return constraint\n",
    "        \n",
    "    def set_LowerSimple(self,\n",
    "                        xi, gi, fi,\n",
    "                        xj, gj, fj,\n",
    "                       ):\n",
    "        \n",
    "        constraint = (fi - fj >= gi**2 / 2 / self.L)\n",
    "\n",
    "        return constraint\n",
    "        \n",
    "    def set_ImprovedSmoothness(self,\n",
    "                             xi, gi, fi,\n",
    "                             xj, gj, fj,\n",
    "                            ):\n",
    "        const = (self.L+self.mu)*(1-self.alpha)**2 / ((self.L+self.mu)*(1-self.alpha)**2-(self.L-self.mu))\n",
    "\n",
    "        constraint = (fi - fj >= 1/4 * (gi + gj) * (xi - xj) + 1 / (4 * self.L) * (gj - gi) ** 2 - self.L/4 * (xj - xi)**2\n",
    "                      + self.alpha / ( 1 - self.alpha ) * ( (fj + gj**2 / 2/self.L) - self.L/4 * const * (xj - xi + (gi+gj)/self.L)**2 ) )\n",
    "\n",
    "        return constraint\n",
    "        \n",
    "    def set_SmoothnessSimple(self,\n",
    "                             xi, gi, fi,\n",
    "                             xj, gj, fj,\n",
    "                            ):\n",
    "        constraint = (fi - fj >= 1/4 * (gi + gj) * (xi - xj) + 1 / (4 * self.L) * (gj - gi) ** 2 - self.L/4 * (xj - xi)**2 )\n",
    "\n",
    "        return constraint\n",
    "    \n",
    "    def add_class_constraints(self):\n",
    "        if self.list_of_stationary_points == list():\n",
    "            self.stationary_point()\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_stationary_points,\n",
    "                                                      constraint_name=\"basic_Lojasiewicz\",\n",
    "                                                      set_class_constraint_i_j=self.set_LojaSimple,\n",
    "                                                      )\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_stationary_points,\n",
    "                                                      constraint_name=\"lower_bound\",\n",
    "                                                      set_class_constraint_i_j=self.set_LowerSimple,\n",
    "                                                      )\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_points,\n",
    "                                                      constraint_name=\"smoothness\",\n",
    "                                                      set_class_constraint_i_j=self.set_SmoothnessSimple,\n",
    "                                                      )\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_points,\n",
    "                                                      constraint_name=\"smoothness\",\n",
    "                                                      set_class_constraint_i_j=self.set_ImprovedSmoothness,\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1f525b-4b5a-4039-afcd-91fab259e0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class NaiveLojasiewiczSmoothConvexFunction(Function):\n",
    "    def __init__(self,\n",
    "                 L,\n",
    "                 mu,\n",
    "                 is_leaf=True,\n",
    "                 decomposition_dict=None,\n",
    "                 reuse_gradient=True,\n",
    "                 name=None):\n",
    "        \n",
    "        super().__init__(is_leaf=is_leaf,\n",
    "                         decomposition_dict=decomposition_dict,\n",
    "                         reuse_gradient=True,\n",
    "                         name=name,\n",
    "                         )\n",
    "        assert L >= 0\n",
    "        assert mu >= 0\n",
    "        assert L >= mu\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.L = L\n",
    "        \n",
    "    def set_LojaSimple(self,\n",
    "                       xi, gi, fi,\n",
    "                       xj, gj, fj,\n",
    "                      ):\n",
    "        \n",
    "        constraint = (fi - fj <= gi**2 / 2 / self.mu)\n",
    "\n",
    "        return constraint\n",
    "    \n",
    "    def set_LowerSimple(self,\n",
    "                        xi, gi, fi,\n",
    "                        xj, gj, fj,\n",
    "                       ):\n",
    "        \n",
    "        \n",
    "        constraint = (fi - fj >= 0)\n",
    "\n",
    "        return constraint\n",
    "    \n",
    "    def set_SmoothnessSimple(self,\n",
    "                             xi, gi, fi,\n",
    "                             xj, gj, fj,\n",
    "                            ):\n",
    "        \n",
    "        constraint = (fi - fj >= 1/4 * (gi + gj) * (xi - xj) + 1 / (4 * self.L) * (gj - gi) ** 2 - self.L/4 * (xj - xi)**2 )\n",
    "\n",
    "        return constraint\n",
    "    \n",
    "    def add_class_constraints(self):\n",
    "        if self.list_of_stationary_points == list():\n",
    "            self.stationary_point()\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_stationary_points,\n",
    "                                                      constraint_name=\"basic_Lojasiewicz\",\n",
    "                                                      set_class_constraint_i_j=self.set_LojaSimple,\n",
    "                                                      )\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_stationary_points,\n",
    "                                                      constraint_name=\"lower_bound\",\n",
    "                                                      set_class_constraint_i_j=self.set_LowerSimple,\n",
    "                                                      )\n",
    "\n",
    "        self.add_constraints_from_two_lists_of_points(list_of_points_1=self.list_of_points,\n",
    "                                                      list_of_points_2=self.list_of_points,\n",
    "                                                      constraint_name=\"smoothness\",\n",
    "                                                      set_class_constraint_i_j=self.set_SmoothnessSimple,\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5056f1-197c-4f74-9fd7-ded3cad3a79e",
   "metadata": {},
   "source": [
    "## Example: gradient descent\n",
    "\n",
    "In this example, we aim to quantify $\\rho$ (the smaller the better) such that the inequality\n",
    "\n",
    "$$ \\min_{0\\leq i\\leq n}\\|\\nabla f(x_{i})\\|^2 \\leq \\tau(L,\\mu,n,\\gamma) \\|x_0-x_\\star\\|^2 $$\n",
    "\n",
    "is valid for all $f$ that is $L$-smooth and satisfy a $\\mu$-Lojasiewicz property, and for all $x_k,x_{k+1}$ such that $x_{k+1}=x_k-\\gamma \\nabla f(x_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74c3355-4bfa-4e5b-9d7c-e6b750387dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothConvexFunction\n",
    "\n",
    "def wc_gradient_descent_NaiveLojaciewicz(L, mu, gamma, n, wrapper=\"cvxpy\", solver=None, verbose=1):\n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a smooth convex function\n",
    "    func = problem.declare_function(NaiveLojasiewiczSmoothConvexFunction, L=L, mu=mu)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_* and corresponding function value fs = f_*\n",
    "    xs = func.stationary_point()\n",
    "    fs = func(xs)\n",
    "\n",
    "    # Then define the starting point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "    g = func.gradient(x0)\n",
    "\n",
    "    # Set the initial constraint that is the distance between x0 and x^*\n",
    "    problem.set_initial_condition( (x0-xs)**2 <= 1)\n",
    "    x = x0\n",
    "    problem.set_performance_metric( g**2 )\n",
    "    for i in range(n):\n",
    "        x = x - gamma * g\n",
    "        g = func.gradient(x)\n",
    "        problem.set_performance_metric( g**2 )\n",
    "        \n",
    "    # Solve the PEP\n",
    "    pepit_verbose = max(verbose, 0)\n",
    "    pepit_tau = problem.solve(wrapper=wrapper, solver=solver, verbose=pepit_verbose)\n",
    "\n",
    "\n",
    "    # Return the worst-case guarantee of the evaluated method (and the reference theoretical value)\n",
    "    return pepit_tau\n",
    "\n",
    "def wc_gradient_descent_ImprovedLojaciewicz(L, mu, gamma, alpha, n, wrapper=\"cvxpy\", solver=None, verbose=1):\n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a smooth convex function\n",
    "    func = problem.declare_function(ImprovedLojasiewiczSmoothConvexFunction, L=L, mu=mu, alpha=alpha)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_* and corresponding function value fs = f_*\n",
    "    xs = func.stationary_point()\n",
    "    fs = func(xs)\n",
    "\n",
    "    # Then define the starting point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "    g = func.gradient(x0)\n",
    "\n",
    "    # Set the initial constraint that is the distance between x0 and x^*\n",
    "    problem.set_initial_condition( (x0-xs)**2 <= 1)\n",
    "    x = x0\n",
    "    problem.set_performance_metric( g**2 )\n",
    "    for i in range(n):\n",
    "        x = x - gamma * g\n",
    "        g = func.gradient(x)\n",
    "        problem.set_performance_metric( g**2 )\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_verbose = max(verbose, 0)\n",
    "    pepit_tau = problem.solve(wrapper=wrapper, solver=solver, verbose=pepit_verbose)\n",
    "\n",
    "\n",
    "    # Return the worst-case guarantee of the evaluated method (and the reference theoretical value)\n",
    "    return pepit_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d462ca3-c7e7-4560-a4aa-be5cfb8d9984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Gradient descent *** \n",
      "\tPEPit guarantee (std inequalities):\t min_(0<=i<=n) ||nabla f(x_(i))||^2_2 <= 0.443727 ||x0-x*||^2_2\n",
      "\tPEPit guarantee (refined inequalities):\t min_(0<=i<=n) ||nabla f(x_(i))||^2_2 <= 0.238939  ||x0-x*||^2_2\n",
      "\tIs the refined inequality stronger? True\n"
     ]
    }
   ],
   "source": [
    "L, mu, gamma, n = 1, .1, 1, 2\n",
    "alpha = (mu/2/(L+mu))\n",
    "verbose = 0\n",
    "\n",
    "pepit_tau_naive = wc_gradient_descent_NaiveLojaciewicz(L, mu, gamma, n, verbose=verbose)\n",
    "pepit_tau_improved = wc_gradient_descent_ImprovedLojaciewicz(L, mu, gamma, alpha, n, verbose=verbose)\n",
    "\n",
    "print('*** Gradient descent *** ')\n",
    "print('\\tPEPit guarantee (std inequalities):\\t min_(0<=i<=n) ||nabla f(x_(i))||^2_2 <= {:.6} ||x0-x*||^2_2'.format(pepit_tau_naive))\n",
    "print('\\tPEPit guarantee (refined inequalities):\\t min_(0<=i<=n) ||nabla f(x_(i))||^2_2 <= {:.6}  ||x0-x*||^2_2'.format(pepit_tau_improved))\n",
    "print('\\tIs the refined inequality stronger? {}'.format(pepit_tau_improved<=pepit_tau_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0e3e7-b37c-46a7-af8b-257bcab2348c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b632df1-25d8-43d0-a445-a240718a010e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
